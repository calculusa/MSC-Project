{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed under terms of the GNU GPL3 license.\n",
    "\n",
    "# urllib is used to download the utils file from deeplearning.net\n",
    "import urllib.request\n",
    "response = urllib.request.urlopen('http://deeplearning.net/tutorial/code/utils.py')\n",
    "content = response.read().decode('utf-8')\n",
    "target = open('utils.py', 'w')\n",
    "target.write(content)\n",
    "target.close()\n",
    "# Import the math function for calculation\n",
    "import math\n",
    "# Tensorflow library. Used to implement machine learning models\n",
    "import tensorflow as tf\n",
    "# Numpy contains helpful functions for efficient mathematical calculations\n",
    "import numpy as np\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from utils import tile_raster_images\n",
    "import pandas as pd\n",
    "\n",
    "# Used to check all the package and environment are installed\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that defines the behavior of the RBM\n",
    "class RBM(object):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Defining the hyperparameters\n",
    "        self._input_size = input_size  # Size of input\n",
    "        self._output_size = output_size  # Size of output\n",
    "        self.epochs = 5  # Amount of training iterations\n",
    "        self.learning_rate = 1.0  # The step used in gradient descent\n",
    "        self.batchsize = 100  # The size of how much data will be used for training per sub iteration\n",
    "\n",
    "        # Initializing weights and biases as matrices full of zeroes\n",
    "        self.w = np.zeros([input_size, output_size], np.float32)  # Creates and initializes the weights with 0\n",
    "        self.hb = np.zeros([output_size], np.float32)  # Creates and initializes the hidden biases with 0\n",
    "        self.vb = np.zeros([input_size], np.float32)  # Creates and initializes the visible biases with 0\n",
    "\n",
    "    # Fits the result from the weighted visible layer plus the bias into a sigmoid curve\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        # Sigmoid\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    # Fits the result from the weighted hidden layer plus the bias into a sigmoid curve\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "\n",
    "    # Generate the sample probability\n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "    # Training method for the model\n",
    "    def train(self, X):\n",
    "        # Create the placeholders for our parameters\n",
    "        _w = tf.placeholder(\"float\", [self._input_size, self._output_size])\n",
    "        _hb = tf.placeholder(\"float\", [self._output_size])\n",
    "        _vb = tf.placeholder(\"float\", [self._input_size])\n",
    "\n",
    "        prv_w = np.zeros([self._input_size, self._output_size],\n",
    "                         np.float32)  # Creates and initializes the weights with 0\n",
    "        prv_hb = np.zeros([self._output_size], np.float32)  # Creates and initializes the hidden biases with 0\n",
    "        prv_vb = np.zeros([self._input_size], np.float32)  # Creates and initializes the visible biases with 0\n",
    "\n",
    "        cur_w = np.zeros([self._input_size, self._output_size], np.float32)\n",
    "        cur_hb = np.zeros([self._output_size], np.float32)\n",
    "        cur_vb = np.zeros([self._input_size], np.float32)\n",
    "        v0 = tf.placeholder(\"float\", [None, self._input_size])\n",
    "\n",
    "        # Initialize with sample probabilities\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "\n",
    "        # Create the Gradients\n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "\n",
    "        # Update learning rates for the layers\n",
    "        update_w = _w + self.learning_rate * (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "        update_vb = _vb + self.learning_rate * tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb + self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "\n",
    "        # Find the error rate\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "\n",
    "        # Training loop\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # For each epoch\n",
    "            for epoch in range(self.epochs):\n",
    "                # For each step/batch\n",
    "                for start, end in zip(range(0, len(X), self.batchsize), range(self.batchsize, len(X), self.batchsize)):\n",
    "                    batch = X[start:end]\n",
    "                    # Update the rates\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print('Epoch: %d' % epoch, 'reconstruction error: %f' % error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "\n",
    "    # Create expected output for our DBN\n",
    "    def rbm_outpt(self, X):\n",
    "        input_X = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        out = tf.nn.sigmoid(tf.matmul(tf.cast(input_X,tf.float32), _w) + _hb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "\n",
    "    def __init__(self, sizes, X, Y):\n",
    "        # Initialize hyperparameters\n",
    "        self._sizes = sizes\n",
    "        self._X = X\n",
    "        self._Y = Y\n",
    "        self.w_list = []\n",
    "        self.b_list = []\n",
    "        self._learning_rate = 1.0\n",
    "        self._momentum = 0.0\n",
    "        self._epoches = 100\n",
    "        self._batchsize = 10\n",
    "        input_size = X.shape[1]\n",
    "\n",
    "        # initialization loop\n",
    "        for size in self._sizes + [Y.shape[1]]:\n",
    "            # Define upper limit for the uniform distribution range\n",
    "            max_range = 4 * math.sqrt(6. / (input_size + size))\n",
    "\n",
    "            # Initialize weights through a random uniform distribution\n",
    "            self.w_list.append(\n",
    "                np.random.uniform(-max_range, max_range, [input_size, size]).astype(np.float32))\n",
    "\n",
    "            # Initialize bias as zeroes\n",
    "            self.b_list.append(np.zeros([size], np.float32))\n",
    "            input_size = size\n",
    "\n",
    "    # load data from rbm\n",
    "    def load_from_rbms(self, dbn_sizes, rbm_list):\n",
    "        # Check if expected sizes are correct\n",
    "        assert len(dbn_sizes) == len(self._sizes)\n",
    "\n",
    "        for i in range(len(self._sizes)):\n",
    "            # Check if for each RBN the expected sizes are correct\n",
    "            assert dbn_sizes[i] == self._sizes[i]\n",
    "\n",
    "        # If everything is correct, bring over the weights and biases\n",
    "        for i in range(len(self._sizes)):\n",
    "            self.w_list[i] = rbm_list[i].w\n",
    "            self.b_list[i] = rbm_list[i].hb\n",
    "\n",
    "    # Training method\n",
    "    def train(self):\n",
    "        # Create placeholders for input, weights, biases, output\n",
    "        _a = [None] * (len(self._sizes) + 2)\n",
    "        _w = [None] * (len(self._sizes) + 1)\n",
    "        _b = [None] * (len(self._sizes) + 1)\n",
    "        _a[0] = tf.placeholder(\"float\", [None, self._X.shape[1]])\n",
    "        y = tf.placeholder(\"float\", [None, self._Y.shape[1]])\n",
    "\n",
    "        # Define variables and activation functoin\n",
    "        for i in range(len(self._sizes) + 1):\n",
    "            _w[i] = tf.Variable(self.w_list[i])\n",
    "            _b[i] = tf.Variable(self.b_list[i])\n",
    "        for i in range(1, len(self._sizes) + 2):\n",
    "            _a[i] = tf.nn.sigmoid(tf.matmul(_a[i - 1], _w[i - 1]) + _b[i - 1])\n",
    "\n",
    "        # Define the cost function\n",
    "        cost = tf.reduce_mean(tf.square(_a[-1] - y))\n",
    "\n",
    "        # Define the training operation (Momentum Optimizer minimizing the Cost function)\n",
    "        train_op = tf.train.MomentumOptimizer(\n",
    "            self._learning_rate, self._momentum).minimize(cost)\n",
    "\n",
    "        # Prediction operation\n",
    "        predict_op = tf.argmax(_a[0], 1)\n",
    "\n",
    "        # Training Loop\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize Variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # For each epoch\n",
    "            for i in range(self._epoches):\n",
    "\n",
    "                # For each step\n",
    "                for start, end in zip(\n",
    "                        range(0, len(self._X), self._batchsize), range(self._batchsize, len(self._X), self._batchsize)):\n",
    "                    # Run the training operation on the input data\n",
    "                    sess.run(train_op, feed_dict={\n",
    "                        _a[0]: self._X[start:end], y: self._Y[start:end]})\n",
    "\n",
    "                for j in range(len(self._sizes) + 1):\n",
    "                    # Retrieve weights and biases\n",
    "                    self.w_list[j] = sess.run(_w[j])\n",
    "                    self.b_list[j] = sess.run(_b[j])\n",
    "\n",
    "                print(\"Accuracy rating for epoch \" + str(i) + \": \" + str(np.mean(np.argmax(self._Y, axis=1) == \\\n",
    "                                                                                 sess.run(predict_op, feed_dict={_a[0]: self._X, y: self._Y}))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    # different stand for the different classification\n",
    "    dataset1 = pd.read_csv('SheffieldProspective_ASL.csv')\n",
    "    #dataset2 = pd.read_csv('SheffieldProspective_sMRI.csv')\n",
    "    dataset3 = pd.read_csv('SheffieldProspective_Demo.csv')\n",
    "    y1 = dataset3.ix[0:73, 2].values\n",
    "    print(y1)\n",
    "    x1 = dataset1.iloc[0:73,1:].values\n",
    "    \n",
    "    # use K-fold to seperate training set and test set\n",
    "    KF=KFold(n_splits=5)  \n",
    "    for train_index,test_index in KF.split(x1):\n",
    "        print(\"TRAIN:\",train_index,\"TEST:\",test_index)\n",
    "    trX,teX=x1[train_index],x1[test_index]\n",
    "    trY=y1[train_index]\n",
    "    teY=y1[test_index]\n",
    "    print(trX,teX)\n",
    "    print(trY,teY)\n",
    "\n",
    "    RBM_hidden_sizes = [25, 15, 15]  # create 4 layers of RBM with size X-25-15-15\n",
    "    # Since we are training, set input as training data\n",
    "    inpX = trX\n",
    "    # Create list to hold our RBMs\n",
    "    rbm_list = []\n",
    "    # Size of inputs is the number of inputs in the training set\n",
    "    input_size = inpX.shape[1]\n",
    "\n",
    "    # For each RBM we want to generate\n",
    "    for i, size in enumerate(RBM_hidden_sizes):\n",
    "        print('RBM: ', i, ' ', input_size, '->', size)\n",
    "        rbm_list.append(RBM(input_size, size))\n",
    "        input_size = size\n",
    "\n",
    "    # For each RBM in our list\n",
    "    for rbm in rbm_list:\n",
    "        print('New RBM:')\n",
    "        # Train a new one\n",
    "        rbm.train(inpX)\n",
    "        # Return the output layer\n",
    "        inpX = rbm.rbm_outpt(inpX)\n",
    "\n",
    "    nNet = NN(RBM_hidden_sizes, trX, trY)\n",
    "    nNet.load_from_rbms(RBM_hidden_sizes, rbm_list)\n",
    "    nNet.train()\n",
    "    \n",
    "    # use confusion matrix to show the training results\n",
    "    y_true = self._Y\n",
    "    y_pred = predict_op\n",
    "    C = confusion_matrix(y_true, y_pred, labels=[\"AD\", \"HC\"])\n",
    "    #C1 = confusion_matrix(y_true, y_pred, labels=[\"MCI\", \"HC\"])\n",
    "    print(C)\n",
    "    #print(C1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
